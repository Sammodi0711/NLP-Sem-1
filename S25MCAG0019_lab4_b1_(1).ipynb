{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sammodi0711/NLP-Sem-1/blob/main/S25MCAG0019_lab4_b1_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DO-rfWjfxsky",
        "outputId": "5aed87e5-4580-4899-dbaa-fb9458d4611a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of sentences: 1\n",
            "Total number of words: 87019\n",
            "Vocabulary size (unique words): 12131\n"
          ]
        }
      ],
      "source": [
        "# Q1. Preprocessing and Tokenization\n",
        "# •\tLoad the news category from the Brown corpus.\n",
        "# •\tClean the text (lowercasing, removing punctuation using re).\n",
        "# •\tTokenize the text into sentences and words.\n",
        "# •\tReport:\n",
        "# 1.\tTotal number of sentences.\n",
        "# 2.\tTotal number of words.\n",
        "# 3.\tVocabulary size (unique words).\n",
        "\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import brown\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "nltk.download('brown')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "news_text = brown.words(categories='news')\n",
        "news_raw = \" \".join(news_text)\n",
        "\n",
        "clean_text = re.sub(r'[^a-zA-Z\\s]', '', news_raw.lower())\n",
        "\n",
        "sentences = sent_tokenize(clean_text)\n",
        "\n",
        "words = word_tokenize(clean_text)\n",
        "\n",
        "total_sentences = len(sentences)\n",
        "total_words = len(words)\n",
        "vocab_size = len(set(words))\n",
        "\n",
        "print(\"Total number of sentences:\", total_sentences)\n",
        "print(\"Total number of words:\", total_words)\n",
        "print(\"Vocabulary size (unique words):\", vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q2. Building N-grams\n",
        "# •\tWrite Python functions to generate unigrams, bigrams, and trigrams from the corpus tokens.\n",
        "# •\tPrint the 10 most frequent bigrams and trigrams along with their counts.\n",
        "\n",
        "from nltk.util import ngrams\n",
        "from nltk import FreqDist\n",
        "\n",
        "news_text = brown.words(categories='news')\n",
        "news_raw = \" \".join(news_text)\n",
        "\n",
        "clean_text = re.sub(r'[^a-zA-Z\\s]', '', news_raw.lower())\n",
        "tokens = word_tokenize(clean_text)\n",
        "\n",
        "def generate_unigrams(tokens):\n",
        "    return list(ngrams(tokens, 1))\n",
        "\n",
        "def generate_bigrams(tokens):\n",
        "    return list(ngrams(tokens, 2))\n",
        "\n",
        "def generate_trigrams(tokens):\n",
        "    return list(ngrams(tokens, 3))\n",
        "\n",
        "unigrams = generate_unigrams(tokens)\n",
        "bigrams = generate_bigrams(tokens)\n",
        "trigrams = generate_trigrams(tokens)\n",
        "\n",
        "bigram_freq = FreqDist(bigrams)\n",
        "trigram_freq = FreqDist(trigrams)\n",
        "\n",
        "print(\"Top 10 Bigrams:\")\n",
        "for pair, count in bigram_freq.most_common(10):\n",
        "    print(pair, \":\", count)\n",
        "\n",
        "print(\"\\nTop 10 Trigrams:\")\n",
        "for triplet, count in trigram_freq.most_common(10):\n",
        "    print(triplet, \":\", count)"
      ],
      "metadata": {
        "id": "NSFTnORV23_-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a01a9cd-ca60-46f5-c242-5afc175421cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 Bigrams:\n",
            "('of', 'the') : 850\n",
            "('in', 'the') : 610\n",
            "('to', 'the') : 279\n",
            "('on', 'the') : 254\n",
            "('for', 'the') : 223\n",
            "('at', 'the') : 199\n",
            "('will', 'be') : 157\n",
            "('that', 'the') : 149\n",
            "('with', 'the') : 142\n",
            "('and', 'the') : 141\n",
            "\n",
            "Top 10 Trigrams:\n",
            "('one', 'of', 'the') : 44\n",
            "('mr', 'and', 'mrs') : 42\n",
            "('the', 'united', 'states') : 37\n",
            "('members', 'of', 'the') : 28\n",
            "('president', 'of', 'the') : 22\n",
            "('a', 'number', 'of') : 19\n",
            "('the', 'white', 'house') : 19\n",
            "('as', 'a', 'result') : 18\n",
            "('some', 'of', 'the') : 18\n",
            "('the', 'u', 's') : 17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q3. Calculating Conditional Probabilities\n",
        "# •\tUsing the Markov Chain assumption and conditional probability, calculate:\n",
        "# o\tP (word₂ | word₁) for bigrams\n",
        "# o\tP (word₃ | word₁, word₂) for trigrams\n",
        "# •\tWrite a function:\n",
        "# def bigram_prob (w1, w2, corpus):\n",
        "#      # returns P(w2|w1)\n",
        "\n",
        "bigrams = list(ngrams(tokens, 2))\n",
        "trigrams = list(ngrams(tokens, 3))\n",
        "\n",
        "unigram_freq = FreqDist(tokens)\n",
        "bigram_freq = FreqDist(bigrams)\n",
        "trigram_freq = FreqDist(trigrams)\n",
        "\n",
        "def bigram_prob(w1, w2):\n",
        "    \"\"\" P(w2 | w1) = Count(w1,w2) / Count(w1) \"\"\"\n",
        "    bigram_count = bigram_freq[(w1, w2)]\n",
        "    unigram_count = unigram_freq[w1]\n",
        "    if unigram_count == 0:\n",
        "        return 0\n",
        "    return bigram_count / unigram_count\n",
        "\n",
        "def trigram_prob(w1, w2, w3):\n",
        "    \"\"\" P(w3 | w1,w2) = Count(w1,w2,w3) / Count(w1,w2) \"\"\"\n",
        "    trigram_count = trigram_freq[(w1, w2, w3)]\n",
        "    bigram_count = bigram_freq[(w1, w2)]\n",
        "    if bigram_count == 0:\n",
        "        return 0\n",
        "    return trigram_count / bigram_count\n",
        "\n",
        "print(\"P('the' | 'in') =\", bigram_prob(\"in\", \"the\"))\n",
        "print(\"P('states' | 'the','united') =\", trigram_prob(\"the\", \"united\", \"states\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdnHOklMKjTB",
        "outputId": "1d5b713c-3cf4-4aa9-9f45-d8b3512eb036"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P('the' | 'in') = 0.30198019801980197\n",
            "P('states' | 'the','united') = 0.74\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q4. Sentence Probability\n",
        "# •\tWrite a function to calculate the probability of a given sentence using:\n",
        "# o\tBigram model\n",
        "# •\tExample sentence: \"the president of the company\"\n",
        "# •\tCompare both results.\n",
        "\n",
        "unigram_freq = FreqDist(tokens)\n",
        "bigram_freq = FreqDist(list(ngrams(tokens, 2)))\n",
        "\n",
        "def bigram_prob(w1, w2):\n",
        "    bigram_count = bigram_freq[(w1, w2)]\n",
        "    unigram_count = unigram_freq[w1]\n",
        "    if unigram_count == 0:\n",
        "        return 0\n",
        "    return bigram_count / unigram_count\n",
        "\n",
        "def sentence_prob_bigram(sentence):\n",
        "    words = word_tokenize(sentence.lower())\n",
        "    prob = 1.0\n",
        "    for i in range(1, len(words)):\n",
        "        p = bigram_prob(words[i-1], words[i])\n",
        "        if p == 0:\n",
        "            return 0\n",
        "        prob *= p\n",
        "    return prob\n",
        "\n",
        "sent = \"the president of the company\"\n",
        "print(\"Sentence:\", sent)\n",
        "print(\"Bigram model probability:\", sentence_prob_bigram(sent))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6gEJ_byPVOD",
        "outputId": "7db6f84a-cbeb-4fb6-cadc-f0a65a4264ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: the president of the company\n",
            "Bigram model probability: 9.05010204014703e-07\n"
          ]
        }
      ]
    }
  ]
}