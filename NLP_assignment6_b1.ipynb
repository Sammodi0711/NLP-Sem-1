{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPgHgn2JiUkaHlXI9RZuOFv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sammodi0711/NLP-Sem-1/blob/main/NLP_assignment6_b1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpL9cB0L3jVf",
        "outputId": "9e088571-dff4-4e20-e99d-2b068d0d5c59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                           text  category                            tokens\n",
            "0          predictable and dull  negative          [predictable, and, dull]\n",
            "1                 waste of time  negative                 [waste, of, time]\n",
            "2        story was good and fun  positive      [story, was, good, and, fun]\n",
            "3  wonderful experience overall  positive  [wonderful, experience, overall]\n",
            "4           acting was terrible  negative           [acting, was, terrible]\n",
            "                                  text  \\\n",
            "0                      very few laughs   \n",
            "1          plain boring and uninspired   \n",
            "2               story was good and fun   \n",
            "3               story was good and fun   \n",
            "4  excellent direction and performance   \n",
            "\n",
            "                                     tokens  \n",
            "0                       [very, few, laughs]  \n",
            "1          [plain, boring, and, uninspired]  \n",
            "2              [story, was, good, and, fun]  \n",
            "3              [story, was, good, and, fun]  \n",
            "4  [excellent, direction, and, performance]  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Dataset:\n",
        "# •\tA training dataset of 100 sentences (provided in CSV format: train.csv), each labeled as positive (+ve) or negative (-ve).\n",
        "# •\tA test dataset of 20 sentences (provided in CSV format: test.csv), without labels.\n",
        "# Tasks:\n",
        "# 1.\tPreprocessing:\n",
        "# o\tUse the training dataset as it is (do not remove stopwords).\n",
        "# o\tTokenize the text into words.\n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "train = pd.read_csv(\"/content/Assignment_6-train_100_wl.csv\")\n",
        "test = pd.read_csv(\"/content/Assignment_6-test_20_wol.csv\")\n",
        "\n",
        "train[\"tokens\"] = train[\"text\"].apply(word_tokenize)\n",
        "test[\"tokens\"] = test[\"text\"].apply(word_tokenize)\n",
        "\n",
        "print(train.head())\n",
        "print(test.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.\tModel Building:\n",
        "# o\tCalculate prior probabilities of each class (+ve and -ve).\n",
        "# o\tCalculate likelihood probabilities of each word given a class using Naïve Bayes formula.\n",
        "\n",
        "import numpy as np\n",
        "import re\n",
        "from collections import defaultdict\n",
        "\n",
        "train = pd.read_csv(\"/content/Assignment_6-train_100_wl.csv\")\n",
        "test = pd.read_csv(\"/content/Assignment_6-test_20_wol.csv\")\n",
        "def tokenize(text):\n",
        "    return re.findall(r\"\\b\\w+\\b\", text.lower())\n",
        "train[\"tokens\"] = train[\"text\"].apply(tokenize)\n",
        "class_counts = train[\"category\"].value_counts()\n",
        "total_docs = len(train)\n",
        "priors = {c: class_counts[c]/total_docs for c in class_counts.index}\n",
        "vocab = set([word for tokens in train[\"tokens\"] for word in tokens])\n",
        "word_counts = {c: defaultdict(int) for c in class_counts.index}\n",
        "class_word_totals = defaultdict(int)\n",
        "\n",
        "for _, row in train.iterrows():\n",
        "    label = row[\"category\"]\n",
        "    for word in row[\"tokens\"]:\n",
        "        word_counts[label][word] += 1\n",
        "        class_word_totals[label] += 1\n",
        "likelihoods = {c: {} for c in class_counts.index}\n",
        "for c in class_counts.index:\n",
        "    for word in vocab:\n",
        "        likelihoods[c][word] = (word_counts[c][word] + 1) / (class_word_totals[c] + len(vocab))\n",
        "\n",
        "print(\"Prior Probabilities:\")\n",
        "print(priors)\n",
        "\n",
        "print(\"\\nLikelihood Probabilities (sample):\")\n",
        "for c in likelihoods:\n",
        "    print(c, dict(list(likelihoods[c].items())[:10]))"
      ],
      "metadata": {
        "id": "9gDGizYu8IhF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b85fcd3-a827-4295-dd27-e840377d24a3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prior Probabilities:\n",
            "{'negative': np.float64(0.5), 'positive': np.float64(0.5)}\n",
            "\n",
            "Likelihood Probabilities (sample):\n",
            "negative {'great': 0.004484304932735426, 'a': 0.004484304932735426, 'powerful': 0.004484304932735426, 'performance': 0.004484304932735426, 'inspiring': 0.004484304932735426, 'amazing': 0.004484304932735426, 'with': 0.004484304932735426, 'enjoyed': 0.004484304932735426, 'movie': 0.017937219730941704, 'wonderful': 0.004484304932735426}\n",
            "positive {'great': 0.01486988847583643, 'a': 0.02973977695167286, 'powerful': 0.02973977695167286, 'performance': 0.01486988847583643, 'inspiring': 0.011152416356877323, 'amazing': 0.01486988847583643, 'with': 0.01486988847583643, 'enjoyed': 0.01858736059479554, 'movie': 0.03345724907063197, 'wonderful': 0.03345724907063197}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.\tPrediction:\n",
        "# o\tApply your Naïve Bayes model on the test dataset.\n",
        "# o\tAssign each test sentence either +ve or -ve sentiment.\n",
        "\n",
        "import math\n",
        "import re\n",
        "train = pd.read_csv(\"/content/Assignment_6-train_100_wl.csv\")\n",
        "test = pd.read_csv(\"/content/Assignment_6-test_20_wol.csv\")\n",
        "pos_count = sum(train['category'] == 'positive')\n",
        "neg_count = sum(train['category'] == 'negative')\n",
        "total_count = len(train)\n",
        "\n",
        "prior_pos = pos_count / total_count\n",
        "prior_neg = neg_count / total_count\n",
        "def tokenize(text):\n",
        "    return re.findall(r\"\\b\\w+\\b\", text.lower())\n",
        "\n",
        "word_counts_pos = {}\n",
        "word_counts_neg = {}\n",
        "total_words_pos = 0\n",
        "total_words_neg = 0\n",
        "\n",
        "for _, row in train.iterrows():\n",
        "    words = tokenize(row['text'])\n",
        "    if row['category'] == 'positive':\n",
        "        for w in words:\n",
        "            word_counts_pos[w] = word_counts_pos.get(w, 0) + 1\n",
        "            total_words_pos += 1\n",
        "    else:\n",
        "        for w in words:\n",
        "            word_counts_neg[w] = word_counts_neg.get(w, 0) + 1\n",
        "            total_words_neg += 1\n",
        "\n",
        "vocab = set(list(word_counts_pos.keys()) + list(word_counts_neg.keys()))\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "def predict(sentence):\n",
        "    words = tokenize(sentence)\n",
        "    log_prob_pos = math.log(prior_pos)\n",
        "    log_prob_neg = math.log(prior_neg)\n",
        "    for w in words:\n",
        "        log_prob_pos += math.log((word_counts_pos.get(w, 0) + 1) / (total_words_pos + vocab_size))\n",
        "        log_prob_neg += math.log((word_counts_neg.get(w, 0) + 1) / (total_words_neg + vocab_size))\n",
        "    return 'positive' if log_prob_pos > log_prob_neg else 'negative'\n",
        "test['prediction'] = test['text'].apply(predict)\n",
        "print(test[['text','prediction']])"
      ],
      "metadata": {
        "id": "0MKce7ukEFLb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c23d245-23e4-48b0-d827-a2d63a600f77"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                   text prediction\n",
            "0                       very few laughs   negative\n",
            "1           plain boring and uninspired   negative\n",
            "2                story was good and fun   positive\n",
            "3                story was good and fun   positive\n",
            "4   excellent direction and performance   positive\n",
            "5        amazing film with great acting   positive\n",
            "6                      not a good story   positive\n",
            "7           plain boring and uninspired   negative\n",
            "8           plain boring and uninspired   negative\n",
            "9   the film was touching and inspiring   positive\n",
            "10       good storyline and nice acting   positive\n",
            "11            movie was boring and slow   negative\n",
            "12             really enjoyed the movie   positive\n",
            "13                        waste of time   negative\n",
            "14                     not a good story   positive\n",
            "15  the film was touching and inspiring   positive\n",
            "16       poor direction and weak script   negative\n",
            "17       loved the characters and story   positive\n",
            "18  excellent direction and performance   positive\n",
            "19          plain boring and uninspired   negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.\tEvaluation:\n",
        "# o\tCalculate Precision, Recall, Accuracy and F1-score without any package.\n",
        "# o\tCalculate Precision, Recall, Accuracy and F1-score with package.\n",
        "# o\tIs there any difference between these two ways of evaluation? If yes, please mention that.\n",
        "\n",
        "y_true = ['+ve', '+ve', '-ve', '+ve', '-ve', '-ve', '+ve', '-ve']\n",
        "y_pred = ['+ve', '-ve', '-ve', '+ve', '-ve', '+ve', '+ve', '-ve']\n",
        "TP = TN = FP = FN = 0\n",
        "\n",
        "for true, pred in zip(y_true, y_pred):\n",
        "    if true == '+ve' and pred == '+ve':\n",
        "        TP += 1\n",
        "    elif true == '-ve' and pred == '-ve':\n",
        "        TN += 1\n",
        "    elif true == '-ve' and pred == '+ve':\n",
        "        FP += 1\n",
        "    elif true == '+ve' and pred == '-ve':\n",
        "        FN += 1\n",
        "\n",
        "precision = TP / (TP + FP) if (TP + FP) != 0 else 0\n",
        "recall = TP / (TP + FN) if (TP + FN) != 0 else 0\n",
        "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
        "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"F1-score:\", f1_score)\n",
        "\n",
        "#with package\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
        "y_true_bin = [1 if label == '+ve' else 0 for label in y_true]\n",
        "y_pred_bin = [1 if label == '+ve' else 0 for label in y_pred]\n",
        "\n",
        "precision = precision_score(y_true_bin, y_pred_bin)\n",
        "recall = recall_score(y_true_bin, y_pred_bin)\n",
        "accuracy = accuracy_score(y_true_bin, y_pred_bin)\n",
        "f1 = f1_score(y_true_bin, y_pred_bin)\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"F1-score:\", f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oPfpB_AT1Vs",
        "outputId": "3a8f3eeb-b873-4650-e403-784307df846b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.75\n",
            "Recall: 0.75\n",
            "Accuracy: 0.75\n",
            "F1-score: 0.75\n",
            "Precision: 0.75\n",
            "Recall: 0.75\n",
            "Accuracy: 0.75\n",
            "F1-score: 0.75\n"
          ]
        }
      ]
    }
  ]
}